{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14393218,"sourceType":"datasetVersion","datasetId":9192300},{"sourceId":288307129,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Run this cell first (single bash cell).\n# Minimal, Kaggle-friendly installs. Adjust versions if Kaggle changes infra.\n!pip install --upgrade pip\n\n# PyTorch - use Kaggle's preinstalled torch if available; else, install a matching wheel.\n# On Kaggle, the preinstalled torch should be fine. If you want specific CUDA wheel, uncomment appropriate line.\n# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# open_clip (LAION implementation)\n!pip install open_clip_torch\n\n# OCR + ASR + client\n!pip install easyocr\n!pip install faster-whisper\n!pip install pillow numpy scipy opencv-python-headless psutil\n\n# OpenAI client (used with Friendli style base_url)\n!pip install openai\n\n# Optional: if you later enable detectron2 / APE, install per repo instructions; heavy.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T00:33:32.875244Z","iopub.execute_input":"2026-01-06T00:33:32.875994Z","iopub.status.idle":"2026-01-06T00:33:52.669370Z","shell.execute_reply.started":"2026-01-06T00:33:32.875957Z","shell.execute_reply":"2026-01-06T00:33:52.668502Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.3\nRequirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.23.0+cu126)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\nRequirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.20)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.1rc0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.11.12)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.2.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\nRequirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\nRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.15.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.2.6)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\nRequirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.7)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.3)\nRequirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.2)\nRequirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.4.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.3)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.10.16)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\nCollecting faster-whisper\n  Downloading faster_whisper-1.2.1-py3-none-any.whl.metadata (16 kB)\nCollecting ctranslate2<5,>=4.0 (from faster-whisper)\n  Downloading ctranslate2-4.6.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: huggingface-hub>=0.21 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.36.0)\nRequirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\nCollecting onnxruntime<2,>=1.14 (from faster-whisper)\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nCollecting av>=11 (from faster-whisper)\n  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.67.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.2.6)\nRequirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.3)\nCollecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (1.2.1rc0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2025.11.12)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\nDownloading faster_whisper-1.2.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading ctranslate2-4.6.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\nInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [faster-whisper]m [onnxruntime]\n\u001b[1A\u001b[2KSuccessfully installed av-16.0.1 coloredlogs-15.0.1 ctranslate2-4.6.2 faster-whisper-1.2.1 humanfriendly-10.0 onnxruntime-1.23.2\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.2.6)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\nRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, sys, json, time, math, base64\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport torch\nimport psutil\n\n# Cache & data dirs (Kaggle working area)\nBASE = Path(\"/kaggle/working/data/cache\")\nFRAMES_DIR = BASE / \"frames\"\nCLIP_DIR = BASE / \"clip_embeddings\"\nAPE_DIR = BASE / \"ape\"\nOCR_DIR = BASE / \"ocr\"\nASR_DIR = BASE / \"asr\"\nRETR_DIR = BASE / \"retrieval\"\nLOG_DIR = BASE / \"logs\"\nVIDEOS = BASE / \"videos\"\n\nfor p in [FRAMES_DIR, CLIP_DIR, APE_DIR, OCR_DIR, ASR_DIR, RETR_DIR, LOG_DIR, VIDEOS]:\n    p.mkdir(parents=True, exist_ok=True)\n\n# Keep HF/torch caches local (avoid filling system cache)\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/hf\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"/kaggle/working/hf\"\nos.environ[\"TORCH_HOME\"] = \"/kaggle/working/torch\"\nos.environ[\"FRIENDLI_API_KEY\"] = \"flp_q0uKwZrrCQKnzUIiqLxul0Nk2qRE3dEvkSBJ9O3hQGw9d\"\n# Friendli/OpenAI key from Kaggle Secrets or env\nFRIENDLI_API_KEY = os.environ.get(\"FRIENDLI_API_KEY\", None)\nif FRIENDLI_API_KEY is None:\n    print(\"WARNING: FRIENDLI_API_KEY not set. Add via Kaggle Secrets or export env var.\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\nprint(\"RAM available (GB):\", psutil.virtual_memory().available/1e9)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:27:28.941602Z","iopub.execute_input":"2026-01-06T01:27:28.942443Z","iopub.status.idle":"2026-01-06T01:27:28.950786Z","shell.execute_reply.started":"2026-01-06T01:27:28.942410Z","shell.execute_reply":"2026-01-06T01:27:28.950199Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nRAM available (GB): 27.915464704\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"os.environ[\"FRIENDLI_API_KEY\"] = \"flp_Od6MuNzJzZQX3tNYu5OntJCKZxTrytnIvErxRJveGyHwd4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:24:39.697381Z","iopub.execute_input":"2026-01-06T01:24:39.697725Z","iopub.status.idle":"2026-01-06T01:24:39.701295Z","shell.execute_reply.started":"2026-01-06T01:24:39.697695Z","shell.execute_reply":"2026-01-06T01:24:39.700689Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import time, json\ndef save_json(obj, path):\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(obj, f, ensure_ascii=False, indent=2)\n    return str(path)\n\ndef load_json(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef save_log(video_name, level, module, msg):\n    p = LOG_DIR / f\"{video_name}.log\"\n    rec = {\"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()), \"level\":level, \"module\":module, \"msg\":msg}\n    with open(p, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(rec) + \"\\n\")\n\ndef np_savez(path, **kwargs):\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    np.savez_compressed(path, **kwargs)\n    return str(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:30.216046Z","iopub.execute_input":"2026-01-06T01:22:30.216712Z","iopub.status.idle":"2026-01-06T01:22:30.222548Z","shell.execute_reply.started":"2026-01-06T01:22:30.216684Z","shell.execute_reply":"2026-01-06T01:22:30.221945Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def sample_frames(video_path, video_name, max_frames=32):\n    out_dir = FRAMES_DIR / video_name\n    out_dir.mkdir(parents=True, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n    if total == 0:\n        raise RuntimeError(\"Video has zero frames or cannot be opened.\")\n    idxs = [min(total-1, int(i * total / max_frames)) for i in range(max_frames)]\n    saved = []\n    for i, idx in enumerate(idxs):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ok, frame = cap.read()\n        if not ok:\n            continue\n        path = out_dir / f\"frame_{i:04d}.jpg\"\n        cv2.imwrite(str(path), frame)  # cv2 writes BGR, but downstream loads expect RGB or convert as needed\n        saved.append(str(path))\n    cap.release()\n    save_log(video_name, \"INFO\", \"frame_sampler\", f\"sampled {len(saved)} frames\")\n    return saved\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:31.895913Z","iopub.execute_input":"2026-01-06T01:22:31.896212Z","iopub.status.idle":"2026-01-06T01:22:31.902246Z","shell.execute_reply.started":"2026-01-06T01:22:31.896186Z","shell.execute_reply":"2026-01-06T01:22:31.901512Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# open_clip usage\nimport open_clip\nfrom PIL import Image\nimport numpy as np\nimport torch\n\ndef run_openclip_and_save(video_name, frame_paths, model_name=\"ViT-L-14\", pretrained=\"laion2b_s32b_b82k\", batch_size=8):\n    \"\"\"\n    Computes image embeddings using open_clip. Saves embeddings as .npz at CLIP_DIR/<video_name>.npz\n    \"\"\"\n    device = DEVICE\n    # model selection fallback: try large then base\n    model_candidates = [(model_name, pretrained), (\"ViT-H-14\", \"laion2b_s32b_b79k\"), (\"ViT-B-32\", \"laion2b_s34b_b79k\")]\n    model = None\n    for name, pre in model_candidates:\n        try:\n            print(f\"Trying open_clip model {name} pretrained={pre} on device={device}\")\n            model, _, preprocess = open_clip.create_model_and_transforms(name, pretrained=pre)\n            tokenizer = open_clip.get_tokenizer(name)\n            model.to(device)\n            model.eval()\n            break\n        except Exception as e:\n            print(f\"Failed to load open_clip model {name} ({pre}): {e}\")\n            model = None\n    if model is None:\n        raise RuntimeError(\"Failed to load any open_clip model\")\n\n    if device == \"cuda\":\n        model = model.half()  # use FP16 on GPU\n\n    all_embs = []\n    frame_idxs = []\n    for i in range(0, len(frame_paths), batch_size):\n        batch_paths = frame_paths[i:i+batch_size]\n        imgs = [preprocess(Image.open(p).convert(\"RGB\")).unsqueeze(0) for p in batch_paths]\n        tensor = torch.cat(imgs, dim=0).to(device)\n        if device == \"cuda\":\n            tensor = tensor.half()\n        with torch.no_grad():\n            img_feats = model.encode_image(tensor)  # shape (B, D)\n            # normalize\n            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n            emb = img_feats.cpu().float().numpy()\n        all_embs.append(emb)\n        frame_idxs.extend(list(range(i, i+len(batch_paths))))\n    embeddings = np.vstack(all_embs)\n    np_savez(CLIP_DIR / f\"{video_name}.npz\", embeddings=embeddings.astype(np.float32), frame_indices=np.array(frame_idxs, dtype=int))\n    save_log(video_name, \"INFO\", \"open_clip\", f\"saved embeddings shape={embeddings.shape}\")\n    # return also tokenizer and model (caller can keep them in memory if desired)\n    return embeddings, frame_idxs, model, tokenizer, preprocess\n\n# Example usage:\n# frames = sample_frames(VIDEO_PATH, VIDEO_NAME, max_frames=32)\n# embeddings, idxs, oc_model, oc_tokenizer, oc_pre = run_openclip_and_save(VIDEO_NAME, frames, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:34.709529Z","iopub.execute_input":"2026-01-06T01:22:34.710322Z","iopub.status.idle":"2026-01-06T01:22:39.245766Z","shell.execute_reply.started":"2026-01-06T01:22:34.710290Z","shell.execute_reply":"2026-01-06T01:22:39.245035Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# !pip install pytesseract","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:02:15.402857Z","iopub.execute_input":"2026-01-06T01:02:15.403084Z","iopub.status.idle":"2026-01-06T01:02:15.406598Z","shell.execute_reply.started":"2026-01-06T01:02:15.403062Z","shell.execute_reply":"2026-01-06T01:02:15.405795Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pytesseract\nfrom PIL import Image\n\ndef run_ocr_and_save(video_name, frame_paths):\n    # Optional: Point to tesseract executable if not in PATH\n    # pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n    \n    out = {}\n    for i, p in enumerate(frame_paths):\n        try:\n            # PyTesseract works best with PIL Image objects\n            img = Image.open(p)\n            # lang='eng' is the default; use config for specific OCR modes\n            text = pytesseract.image_to_string(img, lang='eng')\n            \n            # Splitting by newline to mimic EasyOCR's list output\n            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n            out[i] = lines\n        except Exception as e:\n            out[i] = []\n            \n    save_json({\"frame_idx_to_text\": out}, OCR_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"ocr\", f\"ocr frames processed={len(frame_paths)}\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:39.247111Z","iopub.execute_input":"2026-01-06T01:22:39.247441Z","iopub.status.idle":"2026-01-06T01:22:39.254753Z","shell.execute_reply.started":"2026-01-06T01:22:39.247404Z","shell.execute_reply":"2026-01-06T01:22:39.254040Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from faster_whisper import WhisperModel\ndef run_asr_and_save(video_path, video_name, model_size=\"tiny\"):\n    device = \"cuda\" if DEVICE==\"cuda\" else \"cpu\"\n    compute_type = \"float16\" if (device==\"cuda\") else \"float32\"\n    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n    segments, info = model.transcribe(video_path, beam_size=5, vad_filter=True)\n    segs = []\n    full_text = []\n    for seg in segments:\n        segs.append({\"start\": seg.start, \"end\": seg.end, \"text\": seg.text})\n        full_text.append(seg.text)\n    out = {\"segments\": segs, \"raw_text\": \" \".join(full_text)}\n    save_json(out, ASR_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"asr\", f\"asr segments={len(segs)} model={model_size}\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:39.696876Z","iopub.execute_input":"2026-01-06T01:22:39.697113Z","iopub.status.idle":"2026-01-06T01:22:39.702997Z","shell.execute_reply.started":"2026-01-06T01:22:39.697091Z","shell.execute_reply":"2026-01-06T01:22:39.702339Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# If you have APE/Detectron2 installed & configured, replace the body with actual inference.\n# For modular flow, we only run APE on selected frames (retriever output).\ndef run_ape_on_frames(video_name, selected_frame_paths):\n    # Placeholder: create minimal records for each path\n    out = []\n    for i, p in enumerate(selected_frame_paths):\n        rec = {\"frame_path\": p, \"frame_idx\": int(Path(p).stem.split(\"_\")[-1]), \"objects\": [{\"label\": \"person\", \"bbox\":[0,0,10,10], \"confidence\":0.9}]}\n        out.append(rec)\n    save_json(out, APE_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"ape\", f\"ape stub saved for {len(selected_frame_paths)} frames\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:49.031238Z","iopub.execute_input":"2026-01-06T01:22:49.032020Z","iopub.status.idle":"2026-01-06T01:22:49.036714Z","shell.execute_reply.started":"2026-01-06T01:22:49.031987Z","shell.execute_reply":"2026-01-06T01:22:49.035938Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"fr_model = \"depvgl25ul3x6cv\"\nfr_api = \"flp_Od6MuNzJzZQX3tNYu5OntJCKZxTrytnIvErxRJveGyHwd4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:50.175929Z","iopub.execute_input":"2026-01-06T01:22:50.176225Z","iopub.status.idle":"2026-01-06T01:22:50.180523Z","shell.execute_reply.started":"2026-01-06T01:22:50.176196Z","shell.execute_reply":"2026-01-06T01:22:50.179774Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from openai import OpenAI\n\ndef call_planner_vlm(question):\n    if FRIENDLI_API_KEY is None:\n        raise RuntimeError(\"FRIENDLI_API_KEY not set\")\n    client = OpenAI(api_key=fr_api, base_url=\"https://api.friendli.ai/dedicated/v1\")\n    retrieve_pmt_0 = \"Question: \" + question + \"\\nTo answer the question step by step, provide retrieve request in this JSON format: {\\\"ASR\\\": Optional[str], \\\"DET\\\": Optional[list], \\\"TYPE\\\": Optional[list]}.\\nReturn only valid JSON.\"\n    response = client.chat.completions.create(\n        model=fr_model,\n        messages=[{\"role\":\"user\",\"content\":retrieve_pmt_0}],\n        max_tokens=512,\n        temperature=0.0\n    )\n    planner_text = response.choices[0].message.content\n    save_log(\"planner\", \"INFO\", \"planner_call\", \"got planner response\")\n    return planner_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:52.075794Z","iopub.execute_input":"2026-01-06T01:22:52.076545Z","iopub.status.idle":"2026-01-06T01:22:52.413246Z","shell.execute_reply.started":"2026-01-06T01:22:52.076507Z","shell.execute_reply":"2026-01-06T01:22:52.412515Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# This uses the open_clip model/tokenizer loaded earlier (if you kept it in a long-running session).\n# If not, we will reload a small text encoder variant.\nimport numpy as np\nfrom pathlib import Path\n\ndef retriever_select_frames(video_name, planner_response_text, oc_model=None, oc_tokenizer=None, top_k=4):\n    # parse planner response defensively\n    try:\n        plan = json.loads(planner_response_text)\n    except Exception:\n        plan = {}\n    selected_indices = set()\n\n    # 1) APE mapping (if APE exists)\n    det_labels = plan.get(\"DET\") if isinstance(plan.get(\"DET\"), list) else None\n    ape_file = APE_DIR / f\"{video_name}.json\"\n    if det_labels and ape_file.exists():\n        ape_list = load_json(ape_file)\n        for rec in ape_list:\n            for o in rec.get(\"objects\", []):\n                if o.get(\"label\") in det_labels:\n                    # If rec has frame_idx or frame_path parse it\n                    if \"frame_idx\" in rec:\n                        selected_indices.add(int(rec[\"frame_idx\"]))\n                    else:\n                        # try parse from path\n                        idx = int(Path(rec[\"frame_path\"]).stem.split(\"_\")[-1])\n                        selected_indices.add(idx)\n\n    # 2) CLIP text->image similarity fallback (requires embeddings file)\n    if len(selected_indices) == 0:\n        clip_npz = CLIP_DIR / f\"{video_name}.npz\"\n        if clip_npz.exists():\n            data = np.load(clip_npz)\n            img_embs = data[\"embeddings\"]  # (N, D), already normalized in run_openclip\n            n_frames = img_embs.shape[0]\n            # if planner asked DET labels, embed those labels and compute similarity\n            labels = det_labels or []\n            if len(labels) > 0:\n                # ensure oc_model & tokenizer loaded\n                reload_model = False\n                if oc_model is None or oc_tokenizer is None:\n                    try:\n                        oc_model, _, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n                        oc_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n                        oc_model.to(DEVICE)\n                        if DEVICE == \"cuda\":\n                            oc_model = oc_model.half()\n                        oc_model.eval()\n                        reload_model = True\n                    except Exception as e:\n                        print(\"Failed to load text encoder for fallback:\", e)\n                        oc_model = None\n                        oc_tokenizer = None\n                if oc_model is not None and oc_tokenizer is not None:\n                    try:\n                        toks = oc_tokenizer(labels)  # returns tensor\n                        toks = toks.to(DEVICE)\n                        with torch.no_grad():\n                            text_feats = oc_model.encode_text(toks)\n                            text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n                            t_np = text_feats.cpu().numpy()\n                        img_norm = img_embs / np.linalg.norm(img_embs, axis=1, keepdims=True)\n                        sims = img_norm @ (t_np.T)  # (N, len(labels))\n                        # choose frames with highest max similarity\n                        scores = sims.max(axis=1)\n                        top_idx = np.argsort(scores)[::-1][:top_k]\n                        for idx in top_idx:\n                            selected_indices.add(int(idx))\n                    except Exception as e:\n                        print(\"Text->image sim fallback failed:\", e)\n                # if reload model was True and you want to free memory, you may del oc_model\n                if reload_model:\n                    try:\n                        del oc_model\n                        torch.cuda.empty_cache()\n                    except:\n                        pass\n\n    # 3) final uniform fallback\n    if len(selected_indices) == 0:\n        clip_npz = CLIP_DIR / f\"{video_name}.npz\"\n        if clip_npz.exists():\n            data = np.load(clip_npz)\n            n = data[\"embeddings\"].shape[0]\n            cand = [0, max(0,n//3), max(0,2*n//3), n-1]\n            selected_indices.update([c for c in cand if c < n])\n        else:\n            frames = sorted((FRAMES_DIR / video_name).glob(\"*.jpg\"))\n            selected_indices.update(list(range(min(4, len(frames)))))\n\n    sel = sorted(list(selected_indices))[:top_k]\n    sel_paths = [str(FRAMES_DIR / video_name / f\"frame_{i:04d}.jpg\") for i in sel]\n    out = {\n        \"selected_frame_indices\": sel,\n        \"selected_frame_paths\": sel_paths,\n        \"planner_response\": planner_response_text\n    }\n    save_json(out, RETR_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"retriever\", f\"selected indices: {sel}\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:52.746136Z","iopub.execute_input":"2026-01-06T01:22:52.746749Z","iopub.status.idle":"2026-01-06T01:22:52.759121Z","shell.execute_reply.started":"2026-01-06T01:22:52.746723Z","shell.execute_reply":"2026-01-06T01:22:52.758520Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from openai import OpenAI\n\ndef encode_image_b64(path):\n    with open(path,\"rb\") as f:\n        return base64.b64encode(f.read()).decode(\"utf-8\")\n\ndef call_final_vlm(video_name, question, retrieval_out):\n    if FRIENDLI_API_KEY is None:\n        raise RuntimeError(\"FRIENDLI_API_KEY not set\")\n    client = OpenAI(api_key=fr_api, base_url=\"https://api.friendli.ai/dedicated/v1\")\n    selected_paths = retrieval_out[\"selected_frame_paths\"]\n    content = [{\"type\":\"text\",\"text\": question}]\n    for p in selected_paths:\n        content.append({\"type\":\"image_url\", \"image_url\":{\"url\": f\"data:image/jpeg;base64,{encode_image_b64(p)}\"}})\n    resp = client.chat.completions.create(\n        model=fr_model,\n        messages=[{\"role\":\"user\",\"content\": content}],\n        max_tokens=800,\n        temperature=0.2\n    )\n    answer = resp.choices[0].message.content\n    save_json({\"answer\": answer, \"selected_frame_paths\": selected_paths}, RETR_DIR / f\"{video_name}_final_answer.json\")\n    save_log(video_name, \"INFO\", \"final_vlm\", \"final answer saved\")\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:56.435904Z","iopub.execute_input":"2026-01-06T01:22:56.436206Z","iopub.status.idle":"2026-01-06T01:22:56.442364Z","shell.execute_reply.started":"2026-01-06T01:22:56.436180Z","shell.execute_reply":"2026-01-06T01:22:56.441712Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# planner_resp = call_planner_vlm(QUESTION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:58.756648Z","iopub.execute_input":"2026-01-06T01:22:58.756962Z","iopub.status.idle":"2026-01-06T01:22:58.760150Z","shell.execute_reply.started":"2026-01-06T01:22:58.756934Z","shell.execute_reply":"2026-01-06T01:22:58.759506Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# print(planner_resp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:59.057977Z","iopub.execute_input":"2026-01-06T01:22:59.058229Z","iopub.status.idle":"2026-01-06T01:22:59.061280Z","shell.execute_reply.started":"2026-01-06T01:22:59.058206Z","shell.execute_reply":"2026-01-06T01:22:59.060635Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# !touch '/kaggle/working/data/cache/videos/christmas_tree.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:22:59.796120Z","iopub.execute_input":"2026-01-06T01:22:59.796467Z","iopub.status.idle":"2026-01-06T01:22:59.800239Z","shell.execute_reply.started":"2026-01-06T01:22:59.796420Z","shell.execute_reply":"2026-01-06T01:22:59.799401Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# !cp '/kaggle/input/test-video/christmas_tree.mp4' '/kaggle/working/data/cache/videos/christmas_tree.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:23:00.155894Z","iopub.execute_input":"2026-01-06T01:23:00.156211Z","iopub.status.idle":"2026-01-06T01:23:00.160179Z","shell.execute_reply.started":"2026-01-06T01:23:00.156182Z","shell.execute_reply":"2026-01-06T01:23:00.159379Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Inputs\nVIDEO_PATH = \"/kaggle/working/data/cache/videos/christmas_tree.mp4\"  # change to your video\nVIDEO_NAME = \"christmas_tree\"\nQUESTION = \"How many apples are on the chirstmas tree?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:23:23.716150Z","iopub.execute_input":"2026-01-06T01:23:23.716468Z","iopub.status.idle":"2026-01-06T01:23:23.720180Z","shell.execute_reply.started":"2026-01-06T01:23:23.716425Z","shell.execute_reply":"2026-01-06T01:23:23.719368Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 1) sample frames\nframes = sample_frames(VIDEO_PATH, VIDEO_NAME, max_frames=700)\nprint(\"Sampled frames:\", len(frames))\n\n# 2) CLIP embeddings using open_clip\n# returns model/tokenizer/preprocess too, but we don't keep model global unless needed\nembs, idxs, oc_model, oc_tokenizer, oc_pre = run_openclip_and_save(VIDEO_NAME, frames, model_name=\"ViT-L-14\", pretrained=\"laion2b_s32b_b82k\", batch_size=8)\nprint(\"CLIP embeddings shape:\", embs.shape)\n\n# 3) OCR\nocr_out = run_ocr_and_save(VIDEO_NAME, frames)\nprint(\"OCR done\")\n\n# 4) ASR (small recommended)\nasr_out = run_asr_and_save(VIDEO_PATH, VIDEO_NAME, model_size=\"small\")\nprint(\"ASR done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:36:36.353120Z","iopub.execute_input":"2026-01-06T01:36:36.353767Z","iopub.status.idle":"2026-01-06T01:41:14.148047Z","shell.execute_reply.started":"2026-01-06T01:36:36.353735Z","shell.execute_reply":"2026-01-06T01:41:14.147370Z"}},"outputs":[{"name":"stdout","text":"Sampled frames: 700\nTrying open_clip model ViT-L-14 pretrained=laion2b_s32b_b82k on device=cuda\nCLIP embeddings shape: (700, 768)\nOCR done\nASR done\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"QUESTION = \"How many apples are on the chirstmas tree?\"\n# 5) Planner (remote)\nplanner_resp = call_planner_vlm(QUESTION)\nprint(\"Planner response:\", planner_resp)\n\n# 6) Retriever (uses APE if present else CLIP fallback)\nretrieval = retriever_select_frames(VIDEO_NAME, planner_resp, oc_model=None, oc_tokenizer=None, top_k=4)\nprint(\"Selected frames:\", retrieval[\"selected_frame_paths\"])\n\n# 7) Optionally run APE on selected frames (replace stub with real APE if available)\nape_res = run_ape_on_frames(VIDEO_NAME, retrieval[\"selected_frame_paths\"])\nprint(\"APE (stub) done\")\n\n# 8) Final VLM call\nanswer = call_final_vlm(VIDEO_NAME, QUESTION, retrieval)\nprint(\"FINAL ANSWER:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:41:14.149227Z","iopub.execute_input":"2026-01-06T01:41:14.149511Z","iopub.status.idle":"2026-01-06T01:41:17.037169Z","shell.execute_reply.started":"2026-01-06T01:41:14.149449Z","shell.execute_reply":"2026-01-06T01:41:17.036337Z"}},"outputs":[{"name":"stdout","text":"Planner response: ```json\n{\n  \"ASR\": \"There are 24 apples on the Christmas tree.\",\n  \"DET\": [\"Christmas tree\"],\n  \"TYPE\": [\"Number of apples\"]\n}\n```\nSelected frames: ['/kaggle/working/data/cache/frames/christmas_tree/frame_0000.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0233.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0466.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0699.jpg']\nAPE (stub) done\nFINAL ANSWER:\n The Christmas tree in the video has 6 apples on it.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# NEW QUESTION\nQUESTION = \"What objects are visible near the center of the video?\"\n\nVIDEO_NAME = \"christmas_tree\"\n\n# 1) Planner\nplanner_resp = call_planner_vlm(QUESTION)\nprint(\"Planner response:\\n\", planner_resp)\n\n# 2) Retrieval\nretrieval = retriever_select_frames(\n    VIDEO_NAME,\n    planner_resp,\n    oc_model=None,\n    oc_tokenizer=None,\n    top_k=4\n)\nprint(\"Selected frames:\", retrieval[\"selected_frame_paths\"])\n\n# 3) Final VLM reasoning\nanswer = call_final_vlm(VIDEO_NAME, QUESTION, retrieval)\nprint(\"\\nFINAL ANSWER:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:41:17.038433Z","iopub.execute_input":"2026-01-06T01:41:17.038787Z","iopub.status.idle":"2026-01-06T01:41:21.512250Z","shell.execute_reply.started":"2026-01-06T01:41:17.038737Z","shell.execute_reply":"2026-01-06T01:41:21.511528Z"}},"outputs":[{"name":"stdout","text":"Planner response:\n {\"ASR\": \"a\", \"DET\": [\"a\"], \"TYPE\": [\"object\"]}\nText->image sim fallback failed: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 512 is different from 768)\nSelected frames: ['/kaggle/working/data/cache/frames/christmas_tree/frame_0000.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0233.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0466.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0699.jpg']\n\nFINAL ANSWER:\n The objects visible near the center of the video are:\n\n1. A Christmas tree decorated with ornaments and lights.\n2. A wooden ornament with the \"13abc NEWSNOW\" logo on it.\n3. A drawing of a Christmas tree with people gathered around it.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"QUESTION = \"When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries,which kind of the decoration has the largest number? OPTIONS: A. Apples. B. Candles. C. Berries. D. The three kinds are of the same number.\"\n\nVIDEO_NAME = \"christmas_tree\"\n\n# 1) Planner\nplanner_resp = call_planner_vlm(QUESTION)\nprint(\"Planner response:\\n\", planner_resp)\n\n# 2) Retrieval\nretrieval = retriever_select_frames(\n    VIDEO_NAME,\n    planner_resp,\n    oc_model=None,\n    oc_tokenizer=None,\n    top_k=4\n)\nprint(\"Selected frames:\", retrieval[\"selected_frame_paths\"])\n\n# 3) Final VLM reasoning\nanswer = call_final_vlm(VIDEO_NAME, QUESTION, retrieval)\nprint(\"\\nFINAL ANSWER:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T01:41:21.513607Z","iopub.execute_input":"2026-01-06T01:41:21.513865Z","iopub.status.idle":"2026-01-06T01:41:24.273385Z","shell.execute_reply.started":"2026-01-06T01:41:21.513841Z","shell.execute_reply":"2026-01-06T01:41:24.272592Z"}},"outputs":[{"name":"stdout","text":"Planner response:\n ```json\n{\n  \"ASR\": \"berries\",\n  \"DET\": [\"berries\"],\n  \"TYPE\": [\"apples\", \"candles\", \"berries\"]\n}\n```\nSelected frames: ['/kaggle/working/data/cache/frames/christmas_tree/frame_0000.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0233.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0466.jpg', '/kaggle/working/data/cache/frames/christmas_tree/frame_0699.jpg']\n\nFINAL ANSWER:\n The decoration that has the largest number is the apples.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}