{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14393218,"sourceType":"datasetVersion","datasetId":9192300},{"sourceId":14411763,"sourceType":"datasetVersion","datasetId":9204497},{"sourceId":288307129,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Run this cell first (single bash cell).\n# Minimal, Kaggle-friendly installs. Adjust versions if Kaggle changes infra.\n!pip install --upgrade pip\n\n# PyTorch - use Kaggle's preinstalled torch if available; else, install a matching wheel.\n# On Kaggle, the preinstalled torch should be fine. If you want specific CUDA wheel, uncomment appropriate line.\n# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# open_clip (LAION implementation)\n!pip install open_clip_torch\n\n# OCR + ASR + client\n!pip install easyocr\n!pip install faster-whisper\n!pip install pillow numpy scipy opencv-python-headless psutil\n\n# OpenAI client (used with Friendli style base_url)\n!pip install openai\n\n# Optional: if you later enable detectron2 / APE, install per repo instructions; heavy.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:04.814204Z","iopub.execute_input":"2026-01-06T12:18:04.814447Z","iopub.status.idle":"2026-01-06T12:18:17.502902Z","shell.execute_reply.started":"2026-01-06T12:18:04.814409Z","shell.execute_reply":"2026-01-06T12:18:17.501989Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\nRequirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.23.0+cu126)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\nRequirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.20)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.1rc0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.11.12)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\nRequirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\nRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.15.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.0.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\nRequirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.7)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.3)\nRequirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.2)\nRequirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.4.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.3)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.10.16)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\nRequirement already satisfied: faster-whisper in /usr/local/lib/python3.12/dist-packages (1.2.1)\nRequirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.6.2)\nRequirement already satisfied: huggingface-hub>=0.21 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.36.0)\nRequirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\nRequirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (1.23.2)\nRequirement already satisfied: av>=11 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (16.0.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.67.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.0.2)\nRequirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.3)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21->faster-whisper) (1.2.1rc0)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21->faster-whisper) (2025.11.12)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.15.3)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\nRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, sys, json, time, math, base64\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport torch\nimport psutil\n\n# Cache & data dirs (Kaggle working area)\nBASE = Path(\"/kaggle/working/data/cache\")\nFRAMES_DIR = BASE / \"frames\"\nCLIP_DIR = BASE / \"clip_embeddings\"\nAPE_DIR = BASE / \"ape\"\nOCR_DIR = BASE / \"ocr\"\nASR_DIR = BASE / \"asr\"\nRETR_DIR = BASE / \"retrieval\"\nLOG_DIR = BASE / \"logs\"\nVIDEOS = BASE / \"videos\"\n\nfor p in [FRAMES_DIR, CLIP_DIR, APE_DIR, OCR_DIR, ASR_DIR, RETR_DIR, LOG_DIR, VIDEOS]:\n    p.mkdir(parents=True, exist_ok=True)\n\n# Keep HF/torch caches local (avoid filling system cache)\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/hf\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"/kaggle/working/hf\"\nos.environ[\"TORCH_HOME\"] = \"/kaggle/working/torch\"\nos.environ[\"FRIENDLI_API_KEY\"] = \"flp_q0uKwZrrCQKnzUIiqLxul0Nk2qRE3dEvkSBJ9O3hQGw9d\"\n# Friendli/OpenAI key from Kaggle Secrets or env\nFRIENDLI_API_KEY = os.environ.get(\"FRIENDLI_API_KEY\", None)\nif FRIENDLI_API_KEY is None:\n    print(\"WARNING: FRIENDLI_API_KEY not set. Add via Kaggle Secrets or export env var.\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\nprint(\"RAM available (GB):\", psutil.virtual_memory().available/1e9)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:17.504829Z","iopub.execute_input":"2026-01-06T12:18:17.505141Z","iopub.status.idle":"2026-01-06T12:18:19.163873Z","shell.execute_reply.started":"2026-01-06T12:18:17.505112Z","shell.execute_reply":"2026-01-06T12:18:19.163077Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nRAM available (GB): 31.992336384\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"os.environ[\"FRIENDLI_API_KEY\"] = \"flp_Od6MuNzJzZQX3tNYu5OntJCKZxTrytnIvErxRJveGyHwd4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:19.164987Z","iopub.execute_input":"2026-01-06T12:18:19.165454Z","iopub.status.idle":"2026-01-06T12:18:19.169043Z","shell.execute_reply.started":"2026-01-06T12:18:19.165424Z","shell.execute_reply":"2026-01-06T12:18:19.168381Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import time, json\ndef save_json(obj, path):\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(obj, f, ensure_ascii=False, indent=2)\n    return str(path)\n\ndef load_json(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef save_log(video_name, level, module, msg):\n    p = LOG_DIR / f\"{video_name}.log\"\n    rec = {\"ts\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()), \"level\":level, \"module\":module, \"msg\":msg}\n    with open(p, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(rec) + \"\\n\")\n\ndef np_savez(path, **kwargs):\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    np.savez_compressed(path, **kwargs)\n    return str(path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:19.169871Z","iopub.execute_input":"2026-01-06T12:18:19.170054Z","iopub.status.idle":"2026-01-06T12:18:19.181935Z","shell.execute_reply.started":"2026-01-06T12:18:19.170036Z","shell.execute_reply":"2026-01-06T12:18:19.181396Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def sample_frames(video_path, video_name, max_frames=32):\n    out_dir = FRAMES_DIR / video_name\n    out_dir.mkdir(parents=True, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n    if total == 0:\n        raise RuntimeError(\"Video has zero frames or cannot be opened.\")\n    idxs = [min(total-1, int(i * total / max_frames)) for i in range(max_frames)]\n    saved = []\n    for i, idx in enumerate(idxs):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ok, frame = cap.read()\n        if not ok:\n            continue\n        path = out_dir / f\"frame_{i:04d}.jpg\"\n        cv2.imwrite(str(path), frame)  # cv2 writes BGR, but downstream loads expect RGB or convert as needed\n        saved.append(str(path))\n    cap.release()\n    save_log(video_name, \"INFO\", \"frame_sampler\", f\"sampled {len(saved)} frames\")\n    return saved\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:19.182731Z","iopub.execute_input":"2026-01-06T12:18:19.182984Z","iopub.status.idle":"2026-01-06T12:18:19.192618Z","shell.execute_reply.started":"2026-01-06T12:18:19.182964Z","shell.execute_reply":"2026-01-06T12:18:19.191978Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# open_clip usage\nimport open_clip\nfrom PIL import Image\nimport numpy as np\nimport torch\n\ndef run_openclip_and_save(video_name, frame_paths, model_name=\"ViT-L-14\", pretrained=\"laion2b_s32b_b82k\", batch_size=8):\n    \"\"\"\n    Computes image embeddings using open_clip. Saves embeddings as .npz at CLIP_DIR/<video_name>.npz\n    \"\"\"\n    device = DEVICE\n    # model selection fallback: try large then base\n    model_candidates = [(model_name, pretrained), (\"ViT-H-14\", \"laion2b_s32b_b79k\"), (\"ViT-B-32\", \"laion2b_s34b_b79k\")]\n    model = None\n    for name, pre in model_candidates:\n        try:\n            print(f\"Trying open_clip model {name} pretrained={pre} on device={device}\")\n            model, _, preprocess = open_clip.create_model_and_transforms(name, pretrained=pre)\n            tokenizer = open_clip.get_tokenizer(name)\n            model.to(device)\n            model.eval()\n            break\n        except Exception as e:\n            print(f\"Failed to load open_clip model {name} ({pre}): {e}\")\n            model = None\n    if model is None:\n        raise RuntimeError(\"Failed to load any open_clip model\")\n\n    if device == \"cuda\":\n        model = model.half()  # use FP16 on GPU\n\n    all_embs = []\n    frame_idxs = []\n    for i in range(0, len(frame_paths), batch_size):\n        batch_paths = frame_paths[i:i+batch_size]\n        imgs = [preprocess(Image.open(p).convert(\"RGB\")).unsqueeze(0) for p in batch_paths]\n        tensor = torch.cat(imgs, dim=0).to(device)\n        if device == \"cuda\":\n            tensor = tensor.half()\n        with torch.no_grad():\n            img_feats = model.encode_image(tensor)  # shape (B, D)\n            # normalize\n            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n            emb = img_feats.cpu().float().numpy()\n        all_embs.append(emb)\n        frame_idxs.extend(list(range(i, i+len(batch_paths))))\n    embeddings = np.vstack(all_embs)\n    np_savez(CLIP_DIR / f\"{video_name}.npz\", embeddings=embeddings.astype(np.float32), frame_indices=np.array(frame_idxs, dtype=int))\n    save_log(video_name, \"INFO\", \"open_clip\", f\"saved embeddings shape={embeddings.shape}\")\n    # return also tokenizer and model (caller can keep them in memory if desired)\n    return embeddings, frame_idxs, model, tokenizer, preprocess\n\n# Example usage:\n# frames = sample_frames(VIDEO_PATH, VIDEO_NAME, max_frames=32)\n# embeddings, idxs, oc_model, oc_tokenizer, oc_pre = run_openclip_and_save(VIDEO_NAME, frames, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:19.193478Z","iopub.execute_input":"2026-01-06T12:18:19.193662Z","iopub.status.idle":"2026-01-06T12:18:24.346742Z","shell.execute_reply.started":"2026-01-06T12:18:19.193644Z","shell.execute_reply":"2026-01-06T12:18:24.346061Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# !pip install pytesseract","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.347653Z","iopub.execute_input":"2026-01-06T12:18:24.347892Z","iopub.status.idle":"2026-01-06T12:18:24.351645Z","shell.execute_reply.started":"2026-01-06T12:18:24.347869Z","shell.execute_reply":"2026-01-06T12:18:24.350800Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pytesseract\nfrom PIL import Image\n\ndef run_ocr_and_save(video_name, frame_paths):\n    # Optional: Point to tesseract executable if not in PATH\n    # pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n    \n    out = {}\n    for i, p in enumerate(frame_paths):\n        try:\n            # PyTesseract works best with PIL Image objects\n            img = Image.open(p)\n            # lang='eng' is the default; use config for specific OCR modes\n            text = pytesseract.image_to_string(img, lang='eng')\n            \n            # Splitting by newline to mimic EasyOCR's list output\n            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n            out[i] = lines\n        except Exception as e:\n            out[i] = []\n            \n    save_json({\"frame_idx_to_text\": out}, OCR_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"ocr\", f\"ocr frames processed={len(frame_paths)}\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.354241Z","iopub.execute_input":"2026-01-06T12:18:24.354752Z","iopub.status.idle":"2026-01-06T12:18:24.369114Z","shell.execute_reply.started":"2026-01-06T12:18:24.354729Z","shell.execute_reply":"2026-01-06T12:18:24.368609Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from faster_whisper import WhisperModel\ndef run_asr_and_save(video_path, video_name, model_size=\"tiny\"):\n    device = \"cuda\" if DEVICE==\"cuda\" else \"cpu\"\n    compute_type = \"float16\" if (device==\"cuda\") else \"float32\"\n    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n    segments, info = model.transcribe(video_path, beam_size=5, vad_filter=True)\n    segs = []\n    full_text = []\n    for seg in segments:\n        segs.append({\"start\": seg.start, \"end\": seg.end, \"text\": seg.text})\n        full_text.append(seg.text)\n    out = {\"segments\": segs, \"raw_text\": \" \".join(full_text)}\n    save_json(out, ASR_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"asr\", f\"asr segments={len(segs)} model={model_size}\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.369994Z","iopub.execute_input":"2026-01-06T12:18:24.370289Z","iopub.status.idle":"2026-01-06T12:18:24.409048Z","shell.execute_reply.started":"2026-01-06T12:18:24.370265Z","shell.execute_reply":"2026-01-06T12:18:24.408313Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# If you have APE/Detectron2 installed & configured, replace the body with actual inference.\n# For modular flow, we only run APE on selected frames (retriever output).\ndef run_ape_on_frames(video_name, selected_frame_paths):\n    # Placeholder: create minimal records for each path\n    out = []\n    for i, p in enumerate(selected_frame_paths):\n        rec = {\"frame_path\": p, \"frame_idx\": int(Path(p).stem.split(\"_\")[-1]), \"objects\": [{\"label\": \"person\", \"bbox\":[0,0,10,10], \"confidence\":0.9}]}\n        out.append(rec)\n    save_json(out, APE_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"ape\", f\"ape stub saved for {len(selected_frame_paths)} frames\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.409935Z","iopub.execute_input":"2026-01-06T12:18:24.410159Z","iopub.status.idle":"2026-01-06T12:18:24.414994Z","shell.execute_reply.started":"2026-01-06T12:18:24.410130Z","shell.execute_reply":"2026-01-06T12:18:24.414198Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"fr_model = \"depvgl25ul3x6cv\"\nfr_api = \"flp_Od6MuNzJzZQX3tNYu5OntJCKZxTrytnIvErxRJveGyHwd4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.415959Z","iopub.execute_input":"2026-01-06T12:18:24.416217Z","iopub.status.idle":"2026-01-06T12:18:24.423623Z","shell.execute_reply.started":"2026-01-06T12:18:24.416184Z","shell.execute_reply":"2026-01-06T12:18:24.422986Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from openai import OpenAI\n\ndef call_planner_vlm(question):\n    if FRIENDLI_API_KEY is None:\n        raise RuntimeError(\"FRIENDLI_API_KEY not set\")\n    client = OpenAI(api_key=fr_api, base_url=\"https://api.friendli.ai/dedicated/v1\")\n    retrieve_pmt_0 = \"Question: \" + question + \"\\nTo answer the question step by step, provide retrieve request in this JSON format: {\\\"ASR\\\": Optional[str], \\\"DET\\\": Optional[list], \\\"TYPE\\\": Optional[list]}.\\nReturn only valid JSON.\"\n    response = client.chat.completions.create(\n        model=fr_model,\n        messages=[{\"role\":\"user\",\"content\":retrieve_pmt_0}],\n        max_tokens=512,\n        temperature=0.0\n    )\n    planner_text = response.choices[0].message.content\n    save_log(\"planner\", \"INFO\", \"planner_call\", \"got planner response\")\n    return planner_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.424451Z","iopub.execute_input":"2026-01-06T12:18:24.424761Z","iopub.status.idle":"2026-01-06T12:18:24.990112Z","shell.execute_reply.started":"2026-01-06T12:18:24.424737Z","shell.execute_reply":"2026-01-06T12:18:24.989554Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# This uses the open_clip model/tokenizer loaded earlier (if you kept it in a long-running session).\n# If not, we will reload a small text encoder variant.\nimport numpy as np\nfrom pathlib import Path\n\ndef retriever_select_frames(video_name, planner_response_text, oc_model=None, oc_tokenizer=None, top_k=4):\n    # parse planner response defensively\n    try:\n        plan = json.loads(planner_response_text)\n    except Exception:\n        plan = {}\n    selected_indices = set()\n\n    # 1) APE mapping (if APE exists)\n    det_labels = plan.get(\"DET\") if isinstance(plan.get(\"DET\"), list) else None\n    ape_file = APE_DIR / f\"{video_name}.json\"\n    if det_labels and ape_file.exists():\n        ape_list = load_json(ape_file)\n        for rec in ape_list:\n            for o in rec.get(\"objects\", []):\n                if o.get(\"label\") in det_labels:\n                    # If rec has frame_idx or frame_path parse it\n                    if \"frame_idx\" in rec:\n                        selected_indices.add(int(rec[\"frame_idx\"]))\n                    else:\n                        # try parse from path\n                        idx = int(Path(rec[\"frame_path\"]).stem.split(\"_\")[-1])\n                        selected_indices.add(idx)\n\n    # 2) CLIP text->image similarity fallback (requires embeddings file)\n    if len(selected_indices) == 0:\n        clip_npz = CLIP_DIR / f\"{video_name}.npz\"\n        if clip_npz.exists():\n            data = np.load(clip_npz)\n            img_embs = data[\"embeddings\"]  # (N, D), already normalized in run_openclip\n            n_frames = img_embs.shape[0]\n            # if planner asked DET labels, embed those labels and compute similarity\n            labels = det_labels or []\n            if len(labels) > 0:\n                # ensure oc_model & tokenizer loaded\n                reload_model = False\n                if oc_model is None or oc_tokenizer is None:\n                    try:\n                        oc_model, _, _ = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n                        oc_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n                        oc_model.to(DEVICE)\n                        if DEVICE == \"cuda\":\n                            oc_model = oc_model.half()\n                        oc_model.eval()\n                        reload_model = True\n                    except Exception as e:\n                        print(\"Failed to load text encoder for fallback:\", e)\n                        oc_model = None\n                        oc_tokenizer = None\n                if oc_model is not None and oc_tokenizer is not None:\n                    try:\n                        toks = oc_tokenizer(labels)  # returns tensor\n                        toks = toks.to(DEVICE)\n                        with torch.no_grad():\n                            text_feats = oc_model.encode_text(toks)\n                            text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n                            t_np = text_feats.cpu().numpy()\n                        img_norm = img_embs / np.linalg.norm(img_embs, axis=1, keepdims=True)\n                        sims = img_norm @ (t_np.T)  # (N, len(labels))\n                        # choose frames with highest max similarity\n                        scores = sims.max(axis=1)\n                        top_idx = np.argsort(scores)[::-1][:top_k]\n                        for idx in top_idx:\n                            selected_indices.add(int(idx))\n                    except Exception as e:\n                        print(\"Text->image sim fallback failed:\", e)\n                # if reload model was True and you want to free memory, you may del oc_model\n                if reload_model:\n                    try:\n                        del oc_model\n                        torch.cuda.empty_cache()\n                    except:\n                        pass\n\n    # 3) final uniform fallback\n    if len(selected_indices) == 0:\n        clip_npz = CLIP_DIR / f\"{video_name}.npz\"\n        if clip_npz.exists():\n            data = np.load(clip_npz)\n            n = data[\"embeddings\"].shape[0]\n            cand = [0, max(0,n//3), max(0,2*n//3), n-1]\n            selected_indices.update([c for c in cand if c < n])\n        else:\n            frames = sorted((FRAMES_DIR / video_name).glob(\"*.jpg\"))\n            selected_indices.update(list(range(min(4, len(frames)))))\n\n    sel = sorted(list(selected_indices))[:top_k]\n    sel_paths = [str(FRAMES_DIR / video_name / f\"frame_{i:04d}.jpg\") for i in sel]\n    out = {\n        \"selected_frame_indices\": sel,\n        \"selected_frame_paths\": sel_paths,\n        \"planner_response\": planner_response_text\n    }\n    save_json(out, RETR_DIR / f\"{video_name}.json\")\n    save_log(video_name, \"INFO\", \"retriever\", f\"selected indices: {sel}\")\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:24.991005Z","iopub.execute_input":"2026-01-06T12:18:24.991248Z","iopub.status.idle":"2026-01-06T12:18:25.004527Z","shell.execute_reply.started":"2026-01-06T12:18:24.991225Z","shell.execute_reply":"2026-01-06T12:18:25.003773Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from openai import OpenAI\n\ndef encode_image_b64(path):\n    with open(path,\"rb\") as f:\n        return base64.b64encode(f.read()).decode(\"utf-8\")\n\ndef call_final_vlm(video_name, question, retrieval_out):\n    if FRIENDLI_API_KEY is None:\n        raise RuntimeError(\"FRIENDLI_API_KEY not set\")\n    client = OpenAI(api_key=fr_api, base_url=\"https://api.friendli.ai/dedicated/v1\")\n    selected_paths = retrieval_out[\"selected_frame_paths\"]\n    content = [{\"type\":\"text\",\"text\": question}]\n    for p in selected_paths:\n        content.append({\"type\":\"image_url\", \"image_url\":{\"url\": f\"data:image/jpeg;base64,{encode_image_b64(p)}\"}})\n    resp = client.chat.completions.create(\n        model=fr_model,\n        messages=[{\"role\":\"user\",\"content\": content}],\n        max_tokens=800,\n        temperature=0.2\n    )\n    answer = resp.choices[0].message.content\n    save_json({\"answer\": answer, \"selected_frame_paths\": selected_paths}, RETR_DIR / f\"{video_name}_final_answer.json\")\n    save_log(video_name, \"INFO\", \"final_vlm\", \"final answer saved\")\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.005466Z","iopub.execute_input":"2026-01-06T12:18:25.005760Z","iopub.status.idle":"2026-01-06T12:18:25.018114Z","shell.execute_reply.started":"2026-01-06T12:18:25.005726Z","shell.execute_reply":"2026-01-06T12:18:25.017364Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# planner_resp = call_planner_vlm(QUESTION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.018873Z","iopub.execute_input":"2026-01-06T12:18:25.019093Z","iopub.status.idle":"2026-01-06T12:18:25.029296Z","shell.execute_reply.started":"2026-01-06T12:18:25.019064Z","shell.execute_reply":"2026-01-06T12:18:25.028608Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# print(planner_resp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.030126Z","iopub.execute_input":"2026-01-06T12:18:25.030366Z","iopub.status.idle":"2026-01-06T12:18:25.038789Z","shell.execute_reply.started":"2026-01-06T12:18:25.030322Z","shell.execute_reply":"2026-01-06T12:18:25.038110Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# !touch '/kaggle/working/data/cache/videos/christmas_tree.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.039633Z","iopub.execute_input":"2026-01-06T12:18:25.039882Z","iopub.status.idle":"2026-01-06T12:18:25.049717Z","shell.execute_reply.started":"2026-01-06T12:18:25.039862Z","shell.execute_reply":"2026-01-06T12:18:25.049015Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# !cp '/kaggle/input/test-video/christmas_tree.mp4' '/kaggle/working/data/cache/videos/christmas_tree.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.050475Z","iopub.execute_input":"2026-01-06T12:18:25.050729Z","iopub.status.idle":"2026-01-06T12:18:25.059630Z","shell.execute_reply.started":"2026-01-06T12:18:25.050708Z","shell.execute_reply":"2026-01-06T12:18:25.058982Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"!cp '/kaggle/input/athlete-data/athlete_audio.mp4' '/kaggle/working/data/cache/videos/athlete_audio.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.060518Z","iopub.execute_input":"2026-01-06T12:18:25.061092Z","iopub.status.idle":"2026-01-06T12:18:25.268430Z","shell.execute_reply.started":"2026-01-06T12:18:25.061069Z","shell.execute_reply":"2026-01-06T12:18:25.267455Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # Input 1\n# VIDEO_PATH = \"/kaggle/working/data/cache/videos/christmas_tree.mp4\"  # change to your video\n# VIDEO_NAME = \"christmas_tree\"\n# QUESTION = \"How many apples are on the chirstmas tree?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.269915Z","iopub.execute_input":"2026-01-06T12:18:25.270234Z","iopub.status.idle":"2026-01-06T12:18:25.273846Z","shell.execute_reply.started":"2026-01-06T12:18:25.270203Z","shell.execute_reply":"2026-01-06T12:18:25.273223Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Input 1\nVIDEO_PATH ='/kaggle/working/data/cache/videos/athlete_audio.mp4' # change to your video\nVIDEO_NAME = \"athlete_audio\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.274825Z","iopub.execute_input":"2026-01-06T12:18:25.275103Z","iopub.status.idle":"2026-01-06T12:18:25.283863Z","shell.execute_reply.started":"2026-01-06T12:18:25.275070Z","shell.execute_reply":"2026-01-06T12:18:25.283259Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# 1) sample frames\nframes = sample_frames(VIDEO_PATH, VIDEO_NAME, max_frames=615)\nprint(\"Sampled frames:\", len(frames))\n\n# 2) CLIP embeddings using open_clip\n# returns model/tokenizer/preprocess too, but we don't keep model global unless needed\nembs, idxs, oc_model, oc_tokenizer, oc_pre = run_openclip_and_save(VIDEO_NAME, frames, model_name=\"ViT-L-14\", pretrained=\"laion2b_s32b_b82k\", batch_size=8)\nprint(\"CLIP embeddings shape:\", embs.shape)\n\n# 3) OCR\nocr_out = run_ocr_and_save(VIDEO_NAME, frames)\nprint(\"OCR done\")\n\n# # 4) ASR (small recommended)\n# asr_out = run_asr_and_save(VIDEO_PATH, VIDEO_NAME, model_size=\"small\")\n# print(\"ASR done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:18:25.284685Z","iopub.execute_input":"2026-01-06T12:18:25.284924Z","iopub.status.idle":"2026-01-06T12:20:13.386084Z","shell.execute_reply.started":"2026-01-06T12:18:25.284898Z","shell.execute_reply":"2026-01-06T12:20:13.385300Z"}},"outputs":[{"name":"stdout","text":"Sampled frames: 615\nTrying open_clip model ViT-L-14 pretrained=laion2b_s32b_b82k on device=cuda\nCLIP embeddings shape: (615, 768)\nOCR done\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"QUESTION = \"When and where did the athlete and student first meet in the video?\"\n# 5) Planner (remote)\nplanner_resp = call_planner_vlm(QUESTION)\nprint(\"Planner response:\", planner_resp)\n\n# 6) Retriever (uses APE if present else CLIP fallback)\nretrieval = retriever_select_frames(VIDEO_NAME, planner_resp, oc_model=None, oc_tokenizer=None, top_k=4)\nprint(\"Selected frames:\", retrieval[\"selected_frame_paths\"])\n\n# 7) Optionally run APE on selected frames (replace stub with real APE if available)\nape_res = run_ape_on_frames(VIDEO_NAME, retrieval[\"selected_frame_paths\"])\nprint(\"APE (stub) done\")\n\n# 8) Final VLM call\nanswer = call_final_vlm(VIDEO_NAME, QUESTION, retrieval)\nprint(\"FINAL ANSWER:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:43.044686Z","iopub.execute_input":"2026-01-06T12:32:43.045379Z","iopub.status.idle":"2026-01-06T12:32:44.634969Z","shell.execute_reply.started":"2026-01-06T12:32:43.045322Z","shell.execute_reply":"2026-01-06T12:32:44.634255Z"}},"outputs":[{"name":"stdout","text":"Planner response: ```json\n{\n  \"ASR\": \"The athlete and student first meet in the video at the beginning of the scene.\",\n  \"DET\": [\"the athlete and student\", \"the video\"],\n  \"TYPE\": [\"time\", \"location\"]\n}\n```\nSelected frames: ['/kaggle/working/data/cache/frames/athlete_audio/frame_0000.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0205.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0410.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0614.jpg']\nAPE (stub) done\nFINAL ANSWER:\n The athlete and student first meet in the video at the University of Oregon State.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# NEW QUESTION\nQUESTION = \"When and where did the athlete and student first meet in the video?\"\n\n\n# 1) Planner\nplanner_resp = call_planner_vlm(QUESTION)\nprint(\"Planner response:\\n\", planner_resp)\n\n# 2) Retrieval\nretrieval = retriever_select_frames(\n    VIDEO_NAME,\n    planner_resp,\n    oc_model=None,\n    oc_tokenizer=None,\n    top_k=4\n)\nprint(\"Selected frames:\", retrieval[\"selected_frame_paths\"])\n\n# 3) Final VLM reasoning\nanswer = call_final_vlm(VIDEO_NAME, QUESTION, retrieval)\nprint(\"\\nFINAL ANSWER:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:51.869058Z","iopub.execute_input":"2026-01-06T12:32:51.869686Z","iopub.status.idle":"2026-01-06T12:32:52.971549Z","shell.execute_reply.started":"2026-01-06T12:32:51.869654Z","shell.execute_reply":"2026-01-06T12:32:52.970858Z"}},"outputs":[{"name":"stdout","text":"Planner response:\n ```json\n{\n  \"ASR\": \"The athlete and student first meet in the video\",\n  \"DET\": [\"athlete\", \"student\"],\n  \"TYPE\": [\"first meeting\"]\n}\n```\nSelected frames: ['/kaggle/working/data/cache/frames/athlete_audio/frame_0000.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0205.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0410.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0614.jpg']\n\nFINAL ANSWER:\n The athlete and student first meet in the video at the University of Oregon State, as indicated by the \"Oregon State\" sign on the wall.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"QUESTION = \"When and where did the athlete and student first meet in the video? options: A. 12:30 PM in the car.B. 12:30 PM in the canteen. C. 12:30 PM in the classroom.D. 12:00 PM in the classroom.\"\n\nVIDEO_NAME = \"athlete_audio\"\n\n# 1) Planner\nplanner_resp = call_planner_vlm(QUESTION)\nprint(\"Planner response:\\n\", planner_resp)\n\n# 2) Retrieval\nretrieval = retriever_select_frames(\n    VIDEO_NAME,\n    planner_resp,\n    oc_model=None,\n    oc_tokenizer=None,\n    top_k=4\n)\nprint(\"Selected frames:\", retrieval[\"selected_frame_paths\"])\n\n# 3) Final VLM reasoning\nanswer = call_final_vlm(VIDEO_NAME, QUESTION, retrieval)\nprint(\"\\nFINAL ANSWER:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:35:19.208810Z","iopub.execute_input":"2026-01-06T12:35:19.209534Z","iopub.status.idle":"2026-01-06T12:35:20.268197Z","shell.execute_reply.started":"2026-01-06T12:35:19.209504Z","shell.execute_reply":"2026-01-06T12:35:20.267416Z"}},"outputs":[{"name":"stdout","text":"Planner response:\n ```json\n{\n  \"ASR\": \"12:30 PM in the classroom\",\n  \"DET\": [\"athlete and student\"],\n  \"TYPE\": [\"meeting\"]\n}\n```\nSelected frames: ['/kaggle/working/data/cache/frames/athlete_audio/frame_0000.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0205.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0410.jpg', '/kaggle/working/data/cache/frames/athlete_audio/frame_0614.jpg']\n\nFINAL ANSWER:\n The athlete and student first meet in the video at 12:30 PM in the classroom.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(VIDEO_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:20:15.903431Z","iopub.status.idle":"2026-01-06T12:20:15.903665Z","shell.execute_reply.started":"2026-01-06T12:20:15.903556Z","shell.execute_reply":"2026-01-06T12:20:15.903570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}